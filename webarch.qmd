---
prefer-html: true
editor: 
  markdown: 
    wrap: 72
---

# Web architecture

Dani Arribas-Bel

**Lecture**: The Web's architecture and Economy

**Lab**: What do APIs actually do? (non-spatial APIs)

## Lecture

Slides can be downloaded [here](https://github.com/GDSL-UL/wma/raw/main/pdf/lecture_02.pdf)

## Lab: What do APIs actually do?

In this lab, we will unpack how Application Programming Interfaces ("APIs") work and we cover the basics of accessing an API using R. Instead of downloading a data set, APIs allow programmers, statisticians (or students) to request data directly from a server to a local machine. When you work with web APIs, two different computers --- a client and server --- will interact with each other to request and provide data, respectively.

### RESTful Web APIs are all around you.

**Web APIs**

-   Allow you query a remote database over the internet

-   Take on a variety of formats

-   Adhere to a particular style known as Representation State Transfer or REST (in most cases)

-   RESTful APIs are convenient because we use them to query database using URLs

**Consider a simple Google search:**

![](./img/google.png){width="70%"}

Ever wonder what all that extra stuff in the address bar was all about? In this case, the full address is Google's way of sending a query to its databases requesting information related to the search term *liverpool top attractions*.

![](./img/liv_attractions.png){width="70%"}

In fact, it looks like Google makes its query by taking the search terms, separating each of them with a **+**, and appending them to the link <https://www.google.com/#q=>. Therefore, we should be able to actually change our Google search by adding some terms to the URL:

![](./img/beatles.png){width="70%"}

Learning how to use RESTful APIs is all about learning how to format these URLs so that you can get the response you want.

### Group activity

Get into groups of 5 or 6 students. Using your friend the internet, look up answers to the following questions. Each group will be assigned one question and asked to present their findings in 5 min to discuss with the entire class.

1.  What is a `URL` and how can it help us query data? What is a response status and what are the possible categories?

2.  What is a `GET` request? How does a `GET` request work?

3.  What are API keys and how do you obtain them? What kinds of restrictions to they impose on users? Find an example of an API key, what does it look like?

4.  (For 2 groups) More and more APIs pop up every day. Do a bit of quick research and find 2 different examples of APIs that you would be interested in using. 2 groups, 2 or 3 APIs each.

### API R libraries

There are two ways to collect data through APIs in R:

**Plug-n-play packages.** Many common APIs are available through user-written R Packages. These packages offer functions that conveniently "wrap" API queries and format the response. These packages are usually much more convenient than writing our own query, so it is worth searching for a package that works with the API we need.

**Writing our own API request.** If no wrapper function is available, we have to write our own API request and format the response ourselves using R. This is tricky, but definitely doable.

#### `tidycensus` pair activity 

Some R packages "wrap" API queries and format the response. Lucky us! In pairs, let's have a look at `tidycensus`. You can also have a look at the different APIs available from the [United States Census Bureau](https://www.census.gov/data/developers.html).

`tidycensus is`

`- R package first released in mid-2017`

`- Allows R users to obtain decennial Census and ACS data pre-formatted for use with with tidyverse tools (dplyr, ggplot2, etc.)`

`- Optionally returns geographic data as simple feature geometry for common Census geographies`

Create a new `R-markdown` and save it to something you'll remember, like `web_mapping_lab_02.Rmd`. To get started working, load the package along with the `tidyverse` and `plyr` packages, and set you Census API key. A key can be obtained from http://api.census.gov/data/key_signup.html.

```{r tidycensus}
#| warning: false
library(plyr)
library(tidycensus)
library(tidyverse)

census_api_key("12efa59339e5a00a910f77f2e691309ae70e1d1b") #replace this with your key
```

-   Variables in tidycensus are identified by their Census ID, e.g. B19013_001

-   Entire tables of variables can be requested with the table argument, e.g. table = "B19001"

-   Users can request multiple variables at a time, and set custom names with a named vector

**Searching for variables** Getting variables from the US American Community Survey (ACS) 5-Year Data (2016-2020) requires knowing the variable ID - and there are thousands of these IDs across the different files. To rapidly search for variables, use the load_variables() function. The function takes two required arguments: the year of the Census or endyear of the ACS sample, and the dataset name, which varies in availability by year. For the ACS, use either "acs1" or "acs5" for the ACS detailed tables, and append /profile for the Data Profile and /subject for the Subject Tables. To browse these variables, assign the result of this function to a variable and use the View function in RStudio. An optional argument cache = TRUE will cache the dataset on your computer for future use.

```{r}
#| warning: false
view_vars <- load_variables(2020, "acs5", cache = TRUE)

view(view_vars)
```

**EXERCISE** - In your pairs explore some of the different variables available in the 5-Year ACS (2016-2020). Make a note of 3 variables you would be interested in exploring. The [ACS2 variables page](https://api.census.gov/data/2020/acs/acs5/variables.html) might also help.

```{r}
#| warning: false
income <- get_acs(geography = "state", table = "B19001") #getting income data by state
income
```
**EXERCISE** - 1) What is a tibble? 2) Discuss the format of the data obtained with your partner and then use the function `get_acs` to explore the 3 variables you discussed in the previous exercise.

You can also get "wide" census data:
```{r}
#| warning: false
inc_wide <- get_acs(geography = "state", table = "B19001", output = "wide")
inc_wide
```

Let's make our query a bit more precise. We are going to query data on median household income and median age by county in the state of New York from the 2016-2020 ACS. 

```{r}
#| warning: false
ga_wide <- get_acs(
  geography = "county",
  state = "Louisiana",
  variables = c(medinc = "B19013_001",
                medage = "B01002_001"),
  output = "wide",
  year = 2020
)
```

Let's plot one of our variables. By default, ggplot organizes the data into 30 bins; this option can be changed with the bins parameter.
```{r}
#| warning: false
ggplot(ga_wide, aes(x = medincE)) + 
  geom_histogram(bins = 15) #argument bins = 15 in our call to geom_histogram()
```

We can also easily explort correlations between variables. The `geom_point()` function, which plots points on a chart relative to X and Y values for observations in a dataset. This requires specification of two columns in the call to `aes()`.
```{r}
#| warning: false
#| message: false
ggplot(ga_wide, aes(x = medageE, y = medincE)) + 
  geom_point() + 
  geom_smooth(method = "lm")
```

**EXERCISE** - In your pairs, modify the state, vairables and year parametres in your `get_acs` function and produce some other simple scatter plots (cloud of points) that suggest correlations between your variables of interest.

You can also directly map data you have queried in `tidycensus`. We will look at this in future sessions. For a complete overview of `tidycensus` please see [Analyzing US Census Data: Methods, Maps, and Models in R](https://walker-data.com/census-r/wrangling-census-data-with-tidyverse-tools.html).

#### Your own API request demo

The R libraries that are often used for APIs are `httr` and `jsonlite`. They serve different roles in our introduction of APIs, but both are essential.

`JSON` stands for JavaScript Object Notation. While JavaScript is another programming language. JSON is useful because it is easily readable by a computer, and for this reason, it has become the primary way that data is transported through APIs. Most APIs will send their responses in JSON format. Using the `jsonlite` package, you can extract and format data into an R dataframe. `JSON` is a structure formatted with a key (for example, a variable name `id`) and a value (`BikePoints_308`). We used the function `fromJSON` to transform the API request content into a useable dataframe.

We will request the locations of all the hire bike stations in London from the Transport for London API. We use the `GET` function from `httr` package.The `GET()` function requires a URL, which specifies the server's address to which the request needs to be sent.

```{r pakages API}
#| warning: false
library(httr)
library(jsonlite)

#key <- "YOURKEY HERE"
request <- GET("https://api.tfl.gov.uk/BikePoint/") # Here we request all the bike docking stations from the Transport for London API
```

```{r examine}
#| warning: false
request # Examine output
```

```{r response}
#| warning: false
request$status_code # The response status is 200 for a successful request
```

Most GET request URLs for API querying have three or four components:

1.  Authentication Key/Token: A user-specific character string appended to a base URL telling the server who is making the query; allows servers to efficiently manage database access.

2.  Base URL: A link stub that will be at the beginning of all calls to a given API; points the server to the location of an entire database.

3.  Search Parameters: A character string appended to a base URL that tells the server what to extract from the database; basically a series of filters used to point to specific parts of a database.

4.  Response Format: A character string indicating how the response should be formatted; usually one of .csv, .json, or .xml.

```{r extract}
#| warning: false
bikepoints <- jsonlite::fromJSON(content(request, "text")) # extract the dataframe
names(bikepoints) # Print the column names
bikepoints$`Station ID` = as.numeric(substr(bikepoints$id, nchar("BikePoints_")+1, nchar(bikepoints$id))) # create new ID

```

After Block 3 [Data architectures](https://gdsl-ul.github.io/wma/dataarch.html) we will have revised spatial data forms and you will easily be able to map data that you have obtained through this API.

```{r hirebikestatiosn}
#| warning: false

## Create an sf object from longitude latitude
library(dplyr)
library(sf)
library(tmap)
# create a sf object
stations_df <- bikepoints %>% 
  sf::st_as_sf(coords = c(10,9))  %>%  # create pts from coordinates
  st_set_crs(4326) %>%  # set the original CRS
  relocate(`Station ID`) # set ID as the first column of the dataframe

# plot bikepoints on a background map for more context
tmap_mode("view")
tm_basemap() +
  tm_shape(stations_df) +
  tm_symbols(id = "commonName", col = "red", scale = .5)
```

## Group activity answers

1.  Uniform Resource Location (`URL`) is a string of characters that, when interpreted via the Hypertext Transfer Protocol (`HTTP`). URLs point to a data resource, notably files written in Hypertext Markup Language (`HTML`) or a subset of a database

    -   1xx informational response - the request was received, continuing process

    -   2xx successful - the request was successfully received, understood, and accepted

    -   3xx redirection - further action needs to be taken in order to complete the request

    -   4xx client error - the request contains bad syntax or cannot be fulfilled

    -   5xx server error - the server failed to fulfil an apparently valid request

2.  `GET` requests a representation of a data resource corresponding to a particular `URL`. The process of executing the `GET` method is often referred to as a `GET request` and is the main method used for querying RESTful databases. `HEAD`, `POST`, `PUT`, `DELETE`: other common methods, though mostly never used for database querying.

    Surfing the web is basically equivalent to sending a bunch of `GET` requests to different servers and asking for different files written in `HTM`L. Suppose, for instance, I wanted to look something up on Wikipedia. Your first step would be to open your web browser and type in http://www.wikipedia.org. Once you hit return, you would see the page below. Several different processes occured, however, between you hitting "return" and the page finally being rendered:

    1.  The web browser took the entered character string, used the command-line tool "Curl" to write a properly formatted `HTTP GET` request, and submitted it to the server that hosts the Wikipedia homepage.

    2.  After receiving this request, the server sent an `HTTP` response, from which `Curl` extracted the HTML code for the page (partially shown below).

    3.  The raw `HTML` code was parsed and then executed by the web browser, rendering the page as seen in the window.

    4.  Most APIs requires a key or other user credentials before you can query their database. Getting credentialised with a API requires that you register with the organization. Once you have successfully registered, you will be assigned one or more keys, tokens, or other credentials that must be supplied to the server as part of any API call you make. To make sure users are not abusing their data access privileges (e.g., by making many rapid queries), each set of keys will be given rate limits governing the total number of calls that can be made over certain intervals of time.

3.  Most APIs requires a key before you can query their database. This usually requires you to register with the organization. Most APIs are set up for developers, so you will likely be asked to register an "application." All this really entails is coming up with a name for your app/bot/project and providing your real name, organization, and email. Note that some more popular APIs (e.g., Twitter, Facebook) will require additional information, such as a web address or mobile number. Once you have registered, you will be assigned one or more keys, tokens, or other credentials that must be supplied to the server as part of any API call you make. Most API keys limits he total number of calls that can be made over certain intervals of time. This is so users do not busing their data access privileges.

## References

-   [Brief History of the Internet](https://www.internetsociety.org/resources/doc/2017/brief-history-internet/), by the Internet Society, is a handy (and free!) introduction to how it all came to be.
-   Haklay, M., Singleton, A., Parker, C. 2008. ["Web Mapping 2.0: The Neogeography of the GeoWeb"](https://compass.onlinelibrary.wiley.com/doi/abs/10.1111/j.1749-8198.2008.00167.x). Geography Compass, 2(6):2011--2039
-   [A blog post from Joe Morrison](https://joemorrison.medium.com/death-of-an-open-source-business-model-62bc227a7e9b) commenting on the recent change of licensing for some of the core software from Mapbox
-   Terman, R., 2020. [Computational Tools for Social Science](https://plsc-31101.github.io/course/)
- Walker, K. [Analyzing US Census Data: Methods, Maps, and Models in R](https://walker-data.com/census-r/wrangling-census-data-with-tidyverse-tools.html).
